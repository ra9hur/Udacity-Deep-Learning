{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  f = zipfile.ZipFile(filename)\n",
    "  for name in f.namelist():\n",
    "    return tf.compat.as_str(f.read(name))\n",
    "  f.close()\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits, tf.concat(0, train_labels)))\n",
    "\n",
    "  # Optimizer.\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-43c8b13f6a78>:5 in <module>.: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Average loss at step 0: 3.295773 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.00\n",
      "================================================================================\n",
      "tlewjafjzfmnmeeiptpodenmqchjamrqlq  df s jt rhpbaysztnadmlljrdixevg  wniijoeinkm\n",
      "qzehqhnanegp av ov eoaicw gu  n exf  poohlil albaai ysclahonu i kupgofetgxubnito\n",
      "kfsh g mddorjhilkptnpngsoilboc n kf teadt tkyirwhtiubyq zuowmagxtxap mnqu be jpd\n",
      "ndkcpozinp cnldebvii cat rn rxu njft edohlro sfseomaahovzhkfmozojsrarehs yszgtkh\n",
      "smz xjezaaow m pv prfamaeiepqh ujxe aiewzj abm iamwfinifp vnzpteelnqhseh  iiiukh\n",
      "================================================================================\n",
      "Validation set perplexity: 20.29\n",
      "Average loss at step 100: 2.598737 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.97\n",
      "Validation set perplexity: 10.50\n",
      "Average loss at step 200: 2.251098 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.65\n",
      "Validation set perplexity: 8.45\n",
      "Average loss at step 300: 2.097796 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.37\n",
      "Validation set perplexity: 7.92\n",
      "Average loss at step 400: 2.000738 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.51\n",
      "Validation set perplexity: 7.68\n",
      "Average loss at step 500: 1.935644 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.56\n",
      "Validation set perplexity: 6.97\n",
      "Average loss at step 600: 1.906191 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Validation set perplexity: 7.01\n",
      "Average loss at step 700: 1.859975 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.47\n",
      "Validation set perplexity: 6.74\n",
      "Average loss at step 800: 1.819635 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Validation set perplexity: 6.38\n",
      "Average loss at step 900: 1.828289 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1000: 1.825948 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.66\n",
      "================================================================================\n",
      "f spities woll while use to yoffed in kperementions in ludistely the one there o\n",
      "qugi becool the dud lead offeces the nase of camerablation brea s parcle fam bun\n",
      "lic a ruble a refige refersm be lege free mojen diate edo stiller as the not ks \n",
      "vear was brow kere a the firvs with toermentional arso que soufst in pen precenn\n",
      " retirle lictramjruche kact of persive butole who llection accore appasfish senc\n",
      "================================================================================\n",
      "Validation set perplexity: 6.06\n",
      "Average loss at step 1100: 1.777701 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.82\n",
      "Average loss at step 1200: 1.751987 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1300: 1.733096 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.71\n",
      "Validation set perplexity: 5.68\n",
      "Average loss at step 1400: 1.744443 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.57\n",
      "Average loss at step 1500: 1.738606 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Validation set perplexity: 5.55\n",
      "Average loss at step 1600: 1.742325 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.32\n",
      "Average loss at step 1700: 1.703588 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 5.36\n",
      "Average loss at step 1800: 1.673336 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 1900: 1.639568 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.04\n",
      "Validation set perplexity: 5.17\n",
      "Average loss at step 2000: 1.695407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "ed a similiguation of impistors elegrationed heord to antha boving of jma lawest\n",
      "wesing quadics the having he bela cario texpe is one nine consident and respect \n",
      "inging appola hourcemb and populard it gacons produad adunitite fineions cosne o\n",
      "bust in a most of land it argence a donitaniand differents coont in than complay\n",
      "ets is orqueningoniel the bops arousenrogy less montons who same palvalsuss undi\n",
      "================================================================================\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 2100: 1.682177 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2200: 1.679533 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.44\n",
      "Validation set perplexity: 4.99\n",
      "Average loss at step 2300: 1.639964 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.83\n",
      "Average loss at step 2400: 1.658716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 2500: 1.677405 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.75\n",
      "Average loss at step 2600: 1.650239 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 2700: 1.657594 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 2800: 1.649081 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2900: 1.647330 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3000: 1.650001 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.96\n",
      "================================================================================\n",
      "y one nine seven seven one nine seven one nine seven three jesperems of but accr\n",
      "quall mictomes slavist father s orvedman caquectuy of deble two seven he univers\n",
      "mall of gey to and clatsible and intersity aramard hyow electroged sastents in i\n",
      "ey jathe and compararians games alandinal used pack repuly can beotheramess city\n",
      "auss toarl the perfed incruding ralicist li and intheurg as one five zero two th\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 3100: 1.622756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.74\n",
      "Validation set perplexity: 4.61\n",
      "Average loss at step 3200: 1.643665 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.42\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3300: 1.640119 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3400: 1.665794 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 4.53\n",
      "Average loss at step 3500: 1.658740 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 3600: 1.665547 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3700: 1.648341 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.13\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 3800: 1.645222 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 3900: 1.635769 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 4000: 1.650711 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "================================================================================\n",
      "er x pentico massto of lerej destrimme of de worked rused the sex the first wisi\n",
      "berelz elather alobatic sey hame the greed hoalsh it hiss southiees the greet he\n",
      "d impenysly surse are other broub dozably after indian covenies from place creec\n",
      "ched kovernion is issaz though cloum the one of barn supfemer miverations mapgem\n",
      "hes irredie could dreed all groummed effecr delotage any ftering of hube b augus\n",
      "================================================================================\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4100: 1.634638 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 4200: 1.633756 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 4300: 1.615452 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.44\n",
      "Average loss at step 4400: 1.608619 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 4500: 1.615178 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 4600: 1.614451 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 4700: 1.624013 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 4800: 1.627250 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 4900: 1.627474 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5000: 1.604664 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.42\n",
      "================================================================================\n",
      "betion entertlops fuest bashhin appoirs day boicivanes from but baser ottoman be\n",
      "ovie that convillowofaragy teems that tunly ancounting also petates one nine thr\n",
      " prays is sociated in is to crepadernity are cubstrimation the between the seque\n",
      "th and continueative is will  the end is the coalso the uld the significal g one\n",
      "king for devall defters game en diel the conseurolled a beingoriis worker five s\n",
      "================================================================================\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5100: 1.606695 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 5200: 1.591207 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.32\n",
      "Average loss at step 5300: 1.577751 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5400: 1.579973 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.19\n",
      "Validation set perplexity: 4.29\n",
      "Average loss at step 5500: 1.567574 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "Validation set perplexity: 4.26\n",
      "Average loss at step 5600: 1.583215 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 4.27\n",
      "Average loss at step 5700: 1.571547 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.48\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5800: 1.579880 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Validation set perplexity: 4.25\n",
      "Average loss at step 5900: 1.574613 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 4.23\n",
      "Average loss at step 6000: 1.545791 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "================================================================================\n",
      "isics samald one five lined ie mundombarmua questal diffinite of the be class of\n",
      "phens school s nermips rulagers expeats a region soliral by somile president sec\n",
      "je member was fondred when compynay after include the sstet one nine seven eight\n",
      "pled whose would samp one nine seven such implied by these kingdough bein of rec\n",
      "ther an and parpas feem bost it withineil election froghesed in canging simes be\n",
      "================================================================================\n",
      "Validation set perplexity: 4.21\n",
      "Average loss at step 6100: 1.567285 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.18\n",
      "Average loss at step 6200: 1.533435 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6300: 1.547450 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.15\n",
      "Average loss at step 6400: 1.540972 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.52\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6500: 1.557172 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.60\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6600: 1.594298 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 6700: 1.582711 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Validation set perplexity: 4.19\n",
      "Average loss at step 6800: 1.603450 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Validation set perplexity: 4.17\n",
      "Average loss at step 6900: 1.584817 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.20\n",
      "Average loss at step 7000: 1.579783 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "manencest positionse and recopeds of instersing to crried such as o the emberon \n",
      "d out anno caused for the molity cinise offlestion earl come to the a creakers o\n",
      "bs maceny soming and reperict lixited games sijicop been laudine moctions in was\n",
      "med court of hed unders withwigh resurncthing unocion bully around and patrican \n",
      "with fined moneoc on the eassing a placed in no great home keurch wo the roque e\n",
      "================================================================================\n",
      "Validation set perplexity: 4.18\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables.initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Simple LSTM Model\n",
    "num_nodes = 64\n",
    "\n",
    "# Parameters\n",
    "# Input gate: input, previous output and bias\n",
    "# Single matrix\n",
    "ifcox = tf.Variable(tf.truncated_normal([vocabulary_size, 4*num_nodes], -0.1, 0.1))\n",
    "ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    \n",
    "# Classifier weighs and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "b = tf.Variable(tf.zeros([vocabulary_size]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of cell computation\n",
    "def lstm_cell(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit various connections between\n",
    "    previous state and the gates.\n",
    "    \"\"\"\n",
    "\n",
    "    all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "    update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Input data\n",
    "train_data = list()\n",
    "for _ in range(num_unrollings + 1):\n",
    "    train_data.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "train_inputs = train_data[:num_unrollings]\n",
    "train_labels = train_data[1:]   # labels are inputs shifted by one time step\n",
    "        \n",
    "#Unrolled LSTM loop\n",
    "outputs = list()\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "        \n",
    "# State saving across unrollings\n",
    "with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Classifier            \n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "            \n",
    "# Optimizer\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "    \n",
    "# Predictions\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "# Sampling and validation eval: batch 1, no unrolling\n",
    "sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "    \n",
    "sample_output, sample_state = lstm_cell(sample_input, saved_sample_output, saved_sample_state)\n",
    "    \n",
    "with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.294557 learning rate: 10.000000\n",
      "Minibatch perplexity: 26.97\n",
      "================================================================================\n",
      "eejiw findgtyaq p   tmezieevrpq pax ecofeyoiyukmxrnrm sm r ndkimeoeeyn vmfi fnab\n",
      "b mjjbecr ahrtbgytwdt ol qngeior njnaubaqep bkyetihepjkwh  m veyr suriozomwnf ct\n",
      "i s exepxue mtouru hmjrafn ir moxcjpyqbm tsktdeusexhaner  a exodi spaj dlxbt rlv\n",
      "deonkfl  uecipoahennntho ima  etzih c ftgojueh ixnfog o awsib baze yeehel y o if\n",
      "li o egxdaxoobhnitaydnvukyhlrwzf  ms gbetshoutstkaen ns dkrqgrgem vpnsisafpjinnu\n",
      "================================================================================\n",
      "Validation set perplexity: 20.09\n",
      "Average loss at step 100: 2.631870 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.39\n",
      "Validation set perplexity: 11.72\n",
      "Average loss at step 200: 2.268427 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.32\n",
      "Validation set perplexity: 9.15\n",
      "Average loss at step 300: 2.097086 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Validation set perplexity: 8.07\n",
      "Average loss at step 400: 2.048258 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Validation set perplexity: 7.76\n",
      "Average loss at step 500: 1.994775 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.70\n",
      "Validation set perplexity: 7.28\n",
      "Average loss at step 600: 1.914156 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 6.91\n",
      "Average loss at step 700: 1.887568 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.08\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 800: 1.883212 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Validation set perplexity: 6.79\n",
      "Average loss at step 900: 1.862432 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "Validation set perplexity: 6.34\n",
      "Average loss at step 1000: 1.863945 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.25\n",
      "================================================================================\n",
      "d ply ancually tleaterath raters wein nian not seaviff huw eniesty g in from apt\n",
      "beling aramitices willies onst were jolish in one neneshory xateries harly frove\n",
      "usutibe the marmotter bath usitic and this of dunouses decarred with pa inte wet\n",
      "im air terata trumplical theratist b interning antragi as his oning three rumplo\n",
      "bull aby bety furilidul prodare pootery arilaters euch quriass also repleater ni\n",
      "================================================================================\n",
      "Validation set perplexity: 6.10\n",
      "Average loss at step 1100: 1.816157 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.83\n",
      "Validation set perplexity: 6.29\n",
      "Average loss at step 1200: 1.786265 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.02\n",
      "Validation set perplexity: 6.07\n",
      "Average loss at step 1300: 1.776260 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1400: 1.778899 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.22\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 1500: 1.759735 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.46\n",
      "Average loss at step 1600: 1.745988 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.13\n",
      "Validation set perplexity: 5.65\n",
      "Average loss at step 1700: 1.733757 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.43\n",
      "Average loss at step 1800: 1.709493 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 1900: 1.714426 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 2000: 1.699297 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.63\n",
      "================================================================================\n",
      "bly botablew incluative to pphonainged ann elotoys of onithantly many pererated \n",
      "ne intelled five zero two one nine nine nine clisten the missitions the have att\n",
      "ons indo prefented lograin one nine four exoplicable to fover nine clansyed eigh\n",
      "k fantern mannisen now inccure italyna monyands ittegricuary auchilely hemplogie\n",
      "pler intia instatucor conmdite disected that in tempernene and toon geanchm this\n",
      "================================================================================\n",
      "Validation set perplexity: 5.29\n",
      "Average loss at step 2100: 1.698652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 5.23\n",
      "Average loss at step 2200: 1.720045 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Validation set perplexity: 5.02\n",
      "Average loss at step 2300: 1.723360 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.79\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 2400: 1.698495 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2500: 1.706569 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 4.94\n",
      "Average loss at step 2600: 1.689458 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 2700: 1.702468 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.11\n",
      "Average loss at step 2800: 1.692885 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 2900: 1.692021 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 3000: 1.698613 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "================================================================================\n",
      "xy of the matinus is two one villiandry of the agerum increitle terthill by to t\n",
      "fication splowel rup designal then relation a plived into etculams in now wadted\n",
      "n the batts freshussiabs seven two knotary visses sovers excepted attraishing wh\n",
      "gerated tipe belendial todhenaed zero zero engines achar to parti has lark sksia\n",
      "pedjom indio rancus an ripmehring it in the s arean saba relain works upopule by\n",
      "================================================================================\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3100: 1.667512 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.85\n",
      "Validation set perplexity: 4.90\n",
      "Average loss at step 3200: 1.650436 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Validation set perplexity: 4.95\n",
      "Average loss at step 3300: 1.665585 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3400: 1.648628 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 3500: 1.690880 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 3600: 1.668224 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 3700: 1.670350 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 4.84\n",
      "Average loss at step 3800: 1.673558 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 4.78\n",
      "Average loss at step 3900: 1.671281 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 4000: 1.658074 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.25\n",
      "================================================================================\n",
      " goll of mishmumist faltance is inderection three lave bopsolizs wasar but broat\n",
      "z in the tarman the nopeetry the terover resoritixation one th presidentational \n",
      "zer ornact is under q on a smc mararly political colleges unallocum provegies on\n",
      "ke lorg earngs ng expordion of two zero zero sixin thary my one nine seven two b\n",
      "ry basszing lekess foers seel fourgorial it dendara syst regulary is for a rries\n",
      "================================================================================\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4100: 1.636216 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 4200: 1.632847 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.31\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 4300: 1.637861 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.28\n",
      "Validation set perplexity: 4.71\n",
      "Average loss at step 4400: 1.625201 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.12\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4500: 1.654402 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.24\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 4600: 1.638824 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.88\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 4700: 1.638439 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 4800: 1.621579 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.74\n",
      "Average loss at step 4900: 1.637609 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 5000: 1.625639 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "================================================================================\n",
      "k origince was histolowrequed playerman times drumificates a cost one inco zero \n",
      "rist others we with the one z vist dockementa guan spates demidented from the st\n",
      "hel in exselvoriog that the covinglegito degates freed work akillifrual dipplesi\n",
      "varioungers tar even one two six four of two sceke with wlization v to costummen\n",
      " are metrare with the one nine in trampating san a that the the uf fight k milit\n",
      "================================================================================\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5100: 1.608199 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 5200: 1.609843 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 5300: 1.609196 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.64\n",
      "Validation set perplexity: 4.51\n",
      "Average loss at step 5400: 1.614532 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5500: 1.606945 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.77\n",
      "Validation set perplexity: 4.48\n",
      "Average loss at step 5600: 1.578431 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.58\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 5700: 1.595480 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 5800: 1.615503 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.96\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 5900: 1.601550 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6000: 1.599304 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "================================================================================\n",
      "f the moter in includere beigatician with workering a to gribek that in the fami\n",
      "ican by begin as axays is a his the meatually in this is the econde in is the co\n",
      "leation socarisbort be furring been office family operated force the sex one nin\n",
      "ing that and show oreath to howeed a on the general to impress from some protect\n",
      "ersy this of it cile the bloady ov hattly an a slth brites zero memblers a nexas\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n",
      "Average loss at step 6100: 1.595020 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.52\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 6200: 1.603413 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Validation set perplexity: 4.41\n",
      "Average loss at step 6300: 1.604278 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6400: 1.588430 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6500: 1.575125 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.45\n",
      "Average loss at step 6600: 1.620601 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 6700: 1.586425 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 4.39\n",
      "Average loss at step 6800: 1.594720 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Validation set perplexity: 4.43\n",
      "Average loss at step 6900: 1.587464 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Validation set perplexity: 4.40\n",
      "Average loss at step 7000: 1.607075 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "================================================================================\n",
      "que rripiested alortmanish itrank hardi autient lyancary withing mans the backel\n",
      "line of the share secusing him draphician attract may cobrall how direction prim\n",
      "re forces it of combution system in oee eight predes toed businks and his madesk\n",
      "ement him in climation of the chinishbhatilly modey gragher b we man cloing spec\n",
      "national is often dimin the typ the regard that in one five seven mady currers o\n",
      "================================================================================\n",
      "Validation set perplexity: 4.38\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    print('Initialized')\n",
    "\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        \n",
    "        for i in range(num_unrollings + 1):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "        \n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "\n",
    "            # Mean loss is an estimate of the loss over the last few batches\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step,mean_loss,lr))\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %0.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                \n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "\n",
    "            # Measure validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "\n",
    "            print('Validation set perplexity: %0.2f' % float(np.exp(valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input gate: input, previous output and bias\n",
    "# Single matrix\n",
    "embedding_size = 64\n",
    "num_nodes = 64\n",
    "ifcox = tf.Variable(tf.truncated_normal([embedding_size, 4*num_nodes], -0.1, 0.1))\n",
    "ifcom = tf.Variable(tf.truncated_normal([num_nodes, 4*num_nodes], -0.1, 0.1))\n",
    "ifcob = tf.Variable(tf.zeros([1, 4*num_nodes]))\n",
    "\n",
    "# Parameters:\n",
    "# Variables saving state across unrollings.\n",
    "saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "# Classifier weights and biases.\n",
    "w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "# Defining matrices for: input gate, forget gate, memory cell, output gate\n",
    "m_rows = 4\n",
    "m_input_index = 0\n",
    "m_forget_index = 1\n",
    "m_update_index = 2\n",
    "m_output_index = 3\n",
    "m_input_w = tf.Variable(tf.truncated_normal([m_rows, embedding_size, num_nodes], -0.1, 0.1))\n",
    "m_middle = tf.Variable(tf.truncated_normal([m_rows, num_nodes, num_nodes], -0.1, 0.1))\n",
    "m_biases = tf.Variable(tf.truncated_normal([m_rows, 1, num_nodes], -0.1, 0.1))\n",
    "m_saved_output = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "m_input = tf.Variable(tf.zeros([m_rows, batch_size, num_nodes]), trainable=False)\n",
    "\n",
    "# Variables.\n",
    "embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "# Dropout\n",
    "#  keep_prob = tf.placeholder(tf.float32)\n",
    "keep_prob = 0.75\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Definition of the cell computation.\n",
    "def lstm_cell(i, o, state):\n",
    "    m_input = tf.pack([i for _ in range(m_rows)])\n",
    "    m_saved_output = tf.pack([o for _ in range(m_rows)])\n",
    "\n",
    "    m_input = tf.nn.dropout(m_input, keep_prob)\n",
    "    m_all = tf.batch_matmul(m_input, m_input_w) + tf.batch_matmul(m_saved_output, m_middle) + m_biases\n",
    "    m_all = tf.unpack(m_all)\n",
    "\n",
    "    input_gate = tf.sigmoid(m_all[m_input_index])\n",
    "    forget_gate = tf.sigmoid(m_all[m_forget_index])\n",
    "    update = m_all[m_update_index]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(m_all[m_output_index])\n",
    "\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "# Definition of cell computation\n",
    "def lstm_cell_smart(i, o, state):\n",
    "    \"\"\"\n",
    "    Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit various connections between\n",
    "    previous state and the gates.\n",
    "    \"\"\"\n",
    "\n",
    "    #embed = tf.nn.embedding_lookup(ifcox, i)\n",
    "    i = tf.nn.dropout(i, keep_prob)\n",
    "    all_gates_state = tf.matmul(i, ifcox) + tf.matmul(o, ifcom) + ifcob\n",
    "    input_gate = tf.sigmoid(all_gates_state[:, 0:num_nodes])\n",
    "    forget_gate = tf.sigmoid(all_gates_state[:, num_nodes: 2*num_nodes])\n",
    "    update = all_gates_state[:, 2*num_nodes: 3*num_nodes]\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(all_gates_state[:, 3*num_nodes:])\n",
    "    \n",
    "    return output_gate * tf.tanh(state), state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Input data.\n",
    "train_data = list()\n",
    "train_labels = list()\n",
    "\n",
    "for x in range(num_unrollings):\n",
    "    train_data.append(tf.placeholder(tf.int64, shape=[batch_size]))\n",
    "    train_labels.append(tf.placeholder(tf.float32, shape=[batch_size, vocabulary_size]))\n",
    "\n",
    "    '''\n",
    "    for _ in range(num_unrollings + 1):\n",
    "        train_data.append(tf.placeholder(tf.int64, shape = [batch_size]))\n",
    "    train_labels = train_data[1:]       # labels are inputs shifted by one time step\n",
    "    train_data = train_data[:num_unrollings]\n",
    "    '''\n",
    "    \n",
    "encoded_inputs = list()\n",
    "for bigram_batch in train_data:\n",
    "    embed = tf.nn.embedding_lookup(embeddings, bigram_batch)\n",
    "    encoded_inputs.append(embed)\n",
    "\n",
    "train_inputs = encoded_inputs\n",
    "#  labels = train_labels[1:]\n",
    "\n",
    "# Unrolled LSTM loop.\n",
    "outputs = list()\n",
    "output = saved_output\n",
    "state = saved_state\n",
    "for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    #output, state = lstm_cell_smart(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "# State saving across unrollings.\n",
    "with tf.control_dependencies([saved_output.assign(output), saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(0, outputs), w, b)\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits, tf.concat(0, train_labels)))\n",
    "\n",
    "# Optimizer.\n",
    "global_step = tf.Variable(0)\n",
    "learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "# Predictions\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Sampling and validation eval: batch 1, no unrolling.\n",
    "sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "sample_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "reset_sample_state = tf.group(saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "                                saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "\n",
    "sample_output, sample_state = lstm_cell(sample_embed, saved_sample_output, saved_sample_state)\n",
    "\n",
    "with tf.control_dependencies([saved_sample_output.assign(sample_output), saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.303984 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.06\n",
      "================================================================================\n",
      "a i eh tji shpyex  bheo pn zrybhgperqwa fkvdsei sop umpjahtceiu xzlis  knuee oie\n",
      "chg xirjf y y uuoamyz oi o msnzv ohlan qceycgieirt  lwrobnvsncc eedvutvpokuecjor\n",
      " bfvghnxuupkvafzeiwrys t eia vb jprmn ac h m k uitimefivvyhfvx  hopx a esl neede\n",
      "nstairnzediieaetkqtafrs rrrrt  dlxski oh sirli g xbdjvezj rmcrxiedstkehmhc oallc\n",
      "psuaire ntufgeojrg pe wouidlidbsis wwyeisqaue h f ksljvalfsk qfpdljiky  hijr ehn\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raghu/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:65: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation set perplexity: inf\n",
      "Average loss at step 100: 2.905451 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.29\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 200: 2.897259 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.04\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 300: 2.897594 learning rate: 10.000000\n",
      "Minibatch perplexity: 18.57\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 400: 2.890383 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.99\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 500: 2.889785 learning rate: 10.000000\n",
      "Minibatch perplexity: 16.97\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 600: 2.883024 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.46\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 700: 2.883783 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.18\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 800: 2.881973 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.51\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 900: 2.875092 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.52\n",
      "Validation set perplexity: inf\n",
      "Average loss at step 1000: 2.872634 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.66\n",
      "================================================================================\n",
      "qsereeeiv rais o eiz c eebwp feldahtrano r ootebyiieitasao  f nu    kbealhaeddcm\n",
      "dyaireo timhrhlho io   ysahexiewuittettest pteteanruuooeett   ntsede eodr  neinn\n",
      "otsni latpb  iagls   in  stlntdennoyre lgoysruimhudwiaawiavhrg  oltenngetc etn  \n",
      "ysta wiauciebluapeastiai mak ccs tw   ooofrelde ri oneii ei eefoetle feneea gau \n",
      "mbteoee ekhtwsneeiuhe isb m ehtoltegs  hl nt  rtitwr ttolelflefwr jstrie ii o  r\n",
      "================================================================================\n",
      "Validation set perplexity: inf\n"
     ]
    }
   ],
   "source": [
    "num_steps = 1001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session() as session:\n",
    "    init = tf.global_variables_initializer()\n",
    "    session.run(init)\n",
    "    print('Initialized')\n",
    "\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches_labels = train_batches.next()\n",
    "        batches = [max for max in np.argmax(train_batches.next(), 2)]\n",
    "        feed_dict = dict()\n",
    "\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_data[i]] = batches[i]\n",
    "            feed_dict[train_labels[i]] = batches_labels[i]\n",
    "\n",
    "\n",
    "        _, l, t_predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "\n",
    "            # Mean loss is an estimate of the loss over the last few batches\n",
    "            # training input: 64 X 27\n",
    "            # print('training input: %s' % (feed_dict))\n",
    "            # print('Predictions')\n",
    "            # print(t_predictions)    # 640 X 27\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step,mean_loss,lr))\n",
    "            labels = np.concatenate(list(batches_labels)[1:])\n",
    "            print('Minibatch perplexity: %0.2f' % float(np.exp(logprob(t_predictions, labels))))\n",
    "\n",
    "\n",
    "            # Print every 1000\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "\n",
    "                    for _ in range(79):\n",
    "                        feed = [max for max in np.argmax(feed, 1)]\n",
    "                        prediction = sample_prediction.eval({sample_input: feed})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "\n",
    "\n",
    "            # Measure validation set perplexity\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = [max for max in np.argmax(valid_batches.next(), 2)]\n",
    "                # print('sample input: %s' % (b[0]))\n",
    "                predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "                #print('valid log probability: %f' % (valid_logprob))\n",
    "\n",
    "            print('Validation set perplexity: %0.2f' % float(np.exp(valid_logprob / valid_size)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
